{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3362a434",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/35391/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-4d058ef86e3db8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9600,\n",
       " ('选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般',\n",
       "  1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#Get the data from hugginface\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    # set the parameter split \"train\" for train dataset \"validation\" for validation dataset\n",
    "    def __init__(self, split):\n",
    "        self.dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "        label = self.dataset[i]['label']\n",
    "\n",
    "        return text, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "# show the dataset len and content of 1st element\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4938a3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# use to(device) set the data and model to use cuda\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# get the chinese version tokenizer\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e59695a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " torch.Size([16, 500]),\n",
       " tensor([0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    # sentence\n",
    "    sents = [i[0] for i in data]\n",
    "    # labels\n",
    "    labels = [i[1] for i in data]\n",
    "\n",
    "    # change sentence to tensors\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents, # text\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=500,\n",
    "                                   return_tensors='pt', # pt for PyTorch, or tf for TensorFlow\n",
    "                                   return_length=True)\n",
    "    \n",
    "    # print(data.keys())\n",
    "\n",
    "    #input_ids:map the id to different words\n",
    "    input_ids = data['input_ids']\n",
    "    #attention_mask: words == 1, padding  == 0 [1,1,1,1,1,0,0]\n",
    "    attention_mask = data['attention_mask']\n",
    "    # use for 1st sentence and 2nd sentence [0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "#load the data to torch\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "\n",
    "# for i, (input_ids, attention_mask, token_type_ids,\n",
    "#         labels) in enumerate(loader):\n",
    "#     break\n",
    "\n",
    "# get the first element in loader\n",
    "for input_ids, attention_mask, token_type_ids,labels in loader:\n",
    "    break\n",
    "    \n",
    "# check the loader: loader length is 600, batch is 16, 600*16=9600 rows total\n",
    "print(len(loader))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 500, 768])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# get the pretrained model of bert, need the same version of the tokenizer\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "\n",
    "# don't need train the model and update the params\n",
    "for param in pretrained.parameters():\n",
    "    # print(param.shape)\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# try the first batch, last_hidden_state will give the model's output\n",
    "out = pretrained(input_ids=input_ids.to(device),\n",
    "           attention_mask=attention_mask.to(device),\n",
    "           token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "# [batch_size, sequence_length, hidden_size]\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b8761f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.last_hidden_state[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d3d02a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define forward\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # set the Fully Connected Layer(fc layer)\n",
    "        self.fc = torch.nn.Linear(768, 2).to(device)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # do not need update the weight\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids,\n",
    "                       attention_mask=attention_mask,\n",
    "                       token_type_ids=token_type_ids)\n",
    "\n",
    "        # the 2nd dimension 1st position used to calculate the cls [CLS] 16*768\n",
    "        out = out.last_hidden_state[:, 0]\n",
    "#         print(out.shape)\n",
    "        # 16 768 * 768 * 2 => 16 * 2\n",
    "        out = self.fc(out)\n",
    "#         print(out.shape)\n",
    "\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "model(input_ids=input_ids.to(device),\n",
    "      attention_mask=attention_mask.to(device),\n",
    "      token_type_ids=token_type_ids.to(device)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.6310632824897766 0.6875\n",
      "5 0.6872082948684692 0.5625\n",
      "10 0.6411182284355164 0.625\n",
      "15 0.5849995613098145 0.8125\n",
      "20 0.6454864144325256 0.6875\n",
      "25 0.5975177884101868 0.8125\n",
      "30 0.5190809965133667 0.875\n",
      "35 0.5718966722488403 0.75\n",
      "40 0.5118439793586731 0.9375\n",
      "45 0.6213827729225159 0.75\n",
      "50 0.5946326851844788 0.6875\n",
      "55 0.5772092342376709 0.8125\n",
      "60 0.5248995423316956 0.8125\n",
      "65 0.5566399693489075 0.75\n",
      "70 0.5524975657463074 0.875\n",
      "75 0.4945199191570282 0.875\n",
      "80 0.4463903605937958 0.9375\n",
      "85 0.5500490069389343 0.6875\n",
      "90 0.41821232438087463 1.0\n",
      "95 0.5004081726074219 0.8125\n",
      "100 0.48282232880592346 0.875\n",
      "105 0.5214930772781372 0.6875\n",
      "110 0.4178345799446106 1.0\n",
      "115 0.4676635265350342 0.875\n",
      "120 0.45681968331336975 0.875\n",
      "125 0.5440527200698853 0.8125\n",
      "130 0.4704640507698059 0.875\n",
      "135 0.39664188027381897 1.0\n",
      "140 0.444157212972641 0.9375\n",
      "145 0.4321722984313965 1.0\n",
      "150 0.5079193711280823 0.75\n",
      "155 0.4318912625312805 0.875\n",
      "160 0.43611329793930054 0.9375\n",
      "165 0.4803919196128845 0.875\n",
      "170 0.4719085991382599 0.875\n",
      "175 0.5163881182670593 0.8125\n",
      "180 0.39918801188468933 1.0\n",
      "185 0.4730278253555298 0.9375\n",
      "190 0.5958531498908997 0.6875\n",
      "195 0.4516561031341553 0.875\n",
      "200 0.4921954572200775 0.8125\n",
      "205 0.42956990003585815 0.875\n",
      "210 0.4502459168434143 0.875\n",
      "215 0.40184786915779114 0.9375\n",
      "220 0.44790369272232056 0.875\n",
      "225 0.4577237367630005 0.9375\n",
      "230 0.41704878211021423 0.9375\n",
      "235 0.45730552077293396 0.875\n",
      "240 0.4795500636100769 0.8125\n",
      "245 0.3912377953529358 0.9375\n",
      "250 0.42676660418510437 0.875\n",
      "255 0.4823567569255829 0.875\n",
      "260 0.483185350894928 0.8125\n",
      "265 0.44573166966438293 0.875\n",
      "270 0.4251689910888672 0.875\n",
      "275 0.47137632966041565 0.875\n",
      "280 0.499361515045166 0.75\n",
      "285 0.36340436339378357 1.0\n",
      "290 0.3851739466190338 1.0\n",
      "295 0.4441368877887726 0.875\n",
      "300 0.39159029722213745 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# train optimizer = AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "# define the loss\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids.to(device),\n",
    "                attention_mask=attention_mask.to(device),\n",
    "                token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "    loss = criterion(out, labels.to(device)).to(device)\n",
    "    \n",
    "    # backforward\n",
    "    loss.backward()\n",
    "    \n",
    "    # use the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    # reset the optimizer step to zero, can be put in front of backward\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # every 5 batch show the loss and accuracy\n",
    "    if i % 5 == 0:\n",
    "        # softmax => get the max value index\n",
    "        out = out.argmax(dim=1)\n",
    "        # item() used to get the value from tensor   tensor(5) => 5\n",
    "        accuracy = (out == labels.to(device)).sum().item() / len(labels.to(device))\n",
    "\n",
    "        print(i, loss.item(), accuracy)\n",
    "\n",
    "    if i == 300:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a456c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ True, False, False, False,  True])\n",
      "tensor(2)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1,2,3,4,5])\n",
    "b = torch.tensor([1,1,1,3,5])\n",
    "\n",
    "print(a==b)\n",
    "print((a == b).sum())\n",
    "print((a == b).sum().item())\n",
    "accuracy = (a == b).sum().item() / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "275dd1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/35391/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-4d058ef86e3db8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "0.870777027027027\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('validation'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids.to(device),\n",
    "                        attention_mask=attention_mask.to(device),\n",
    "                        token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels.to(device)).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e85425",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
