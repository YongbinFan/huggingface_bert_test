{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3362a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/35391/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-4d058ef86e3db8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Loading cached processed dataset at C:\\Users\\35391\\.cache\\huggingface\\datasets\\lansinuote___parquet\\lansinuote--ChnSentiCorp-4d058ef86e3db8d5\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-1a622b13dfce1ac5.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9192,\n",
       " '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "#Get the data from hugginface\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "\n",
    "        def f(data):\n",
    "            return len(data['text']) > 30\n",
    "        \n",
    "        # only keep the senten length > 30\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "\n",
    "        return text\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "# original dataset size 9600\n",
    "len(dataset), dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e14b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# use to(device) set the data and model to use cuda\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# get the chinese version tokenizer\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e59695a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574\n",
      "[CLS] 房 间 设 施 陈 旧 ， 气 味 太 重 ， 床 单 [MASK] 有 黄 色 斑 点 。 免 费 注 册 网 站 导 [SEP]\n",
      "上\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 30]),\n",
       " torch.Size([16, 30]),\n",
       " torch.Size([16, 30]),\n",
       " torch.Size([16]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    # encode the data\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=data,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=30, # set the sentence to 30 length\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True)\n",
    "\n",
    "    #input_ids:map the id to different words\n",
    "    #attention_mask: words == 1, padding  == 0 [1,1,1,1,1,0,0]\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    \n",
    "    # use for 1st sentence and 2nd sentence [0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "    token_type_ids = data['token_type_ids']\n",
    "\n",
    "    # get the label ids which is the 15th word of the sentence\n",
    "    labels = input_ids[:, 15].reshape(-1).clone()\n",
    "#     print(\"labels\")\n",
    "#     print(labels)\n",
    "    \n",
    "    \n",
    "    # token.mask_token return \"[MASK]\"\n",
    "    # token.get_vocab() return the dict of all word:id in the tokenizer\n",
    "    # token.get_vocab()[token.mask_token] return the \"[MASK]\" key value which is 103\n",
    "    # {...'[SEP]': 102, '[MASK]': 103, '<S>': 104,....}\n",
    "    # input_ids[:, 15] this will set all the 15th word of the sentence to be [MASK]/103\n",
    "    input_ids[:, 15] = token.get_vocab()[token.mask_token]\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "# put the data into torch dataloader\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=16,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    break\n",
    "\n",
    "print(len(loader))\n",
    "print(token.decode(input_ids[0]))\n",
    "print(token.decode(labels[0]))\n",
    "input_ids.shape, attention_mask.shape, token_type_ids.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# get the pretrained model of bert, need the same version of the tokenizer\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "\n",
    "# don't need train the model and update the params\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# try the first batch, last_hidden_state will give the model's output\n",
    "out = pretrained(input_ids=input_ids.to(device),\n",
    "           attention_mask=attention_mask.to(device),\n",
    "           token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b273bf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 21128])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define forward model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # set the Fully Connected Layer(fc layer)\n",
    "        # output willbe the softmax(token.vocab_size) which size should be 21128 in this tokenizer\n",
    "        self.decoder = torch.nn.Linear(768, token.vocab_size, bias=False)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(token.vocab_size))\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        # do not need update the weight\n",
    "        with torch.no_grad():\n",
    "            out = pretrained(input_ids=input_ids.to(device),\n",
    "                             attention_mask=attention_mask.to(device),\n",
    "                             token_type_ids=token_type_ids.to(device))\n",
    "        # get the softmax value of the 15th word\n",
    "        out = self.decoder(out.last_hidden_state[:, 15])\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 10.03921127319336 0.0\n",
      "0 50 8.445544242858887 0.125\n",
      "0 100 5.90747594833374 0.125\n",
      "0 150 5.278145790100098 0.1875\n",
      "0 200 4.775643348693848 0.25\n",
      "0 250 5.233343601226807 0.4375\n",
      "0 300 3.9224624633789062 0.3125\n",
      "0 350 3.90852427482605 0.5\n",
      "0 400 2.863471746444702 0.5\n",
      "0 450 4.3009843826293945 0.25\n",
      "0 500 3.597964286804199 0.5\n",
      "0 550 5.555667400360107 0.375\n",
      "1 0 2.691502094268799 0.6875\n",
      "1 50 2.3754396438598633 0.8125\n",
      "1 100 2.2875373363494873 0.625\n",
      "1 150 2.842221975326538 0.625\n",
      "1 200 3.339266300201416 0.5\n",
      "1 250 1.8524198532104492 0.6875\n",
      "1 300 1.7356739044189453 0.5625\n",
      "1 350 2.6079063415527344 0.5625\n",
      "1 400 2.120748281478882 0.6875\n",
      "1 450 2.364314079284668 0.625\n",
      "1 500 2.830188512802124 0.5625\n",
      "1 550 1.8289223909378052 0.5625\n",
      "2 0 1.304487943649292 0.6875\n",
      "2 50 0.4457993805408478 0.9375\n",
      "2 100 0.901310920715332 0.875\n",
      "2 150 1.272574782371521 0.875\n",
      "2 200 1.1237680912017822 0.75\n",
      "2 250 1.324442982673645 0.8125\n",
      "2 300 0.8867257833480835 0.9375\n",
      "2 350 0.7476117014884949 0.9375\n",
      "2 400 0.7557695508003235 0.9375\n",
      "2 450 1.1747193336486816 0.8125\n",
      "2 500 1.5402554273605347 0.8125\n",
      "2 550 1.1226388216018677 0.6875\n",
      "3 0 0.7732843160629272 0.875\n",
      "3 50 0.8675990104675293 0.875\n",
      "3 100 0.7065597176551819 0.875\n",
      "3 150 0.5104202628135681 0.875\n",
      "3 200 0.47477445006370544 0.875\n",
      "3 250 0.513683021068573 0.875\n",
      "3 300 0.5731051564216614 0.9375\n",
      "3 350 0.6131245493888855 0.8125\n",
      "3 400 0.3186899721622467 1.0\n",
      "3 450 0.7712413668632507 0.8125\n",
      "3 500 0.5549635887145996 0.875\n",
      "3 550 0.7848011255264282 0.75\n",
      "4 0 0.505222499370575 0.9375\n",
      "4 50 0.12391283363103867 1.0\n",
      "4 100 0.43116095662117004 0.9375\n",
      "4 150 0.5397417545318604 0.9375\n",
      "4 200 0.2112853080034256 0.9375\n",
      "4 250 0.1847013533115387 1.0\n",
      "4 300 0.3187320828437805 0.9375\n",
      "4 350 0.8273056149482727 0.75\n",
      "4 400 0.16995775699615479 1.0\n",
      "4 450 0.30471619963645935 0.9375\n",
      "4 500 0.2162638008594513 1.0\n",
      "4 550 0.6841047406196594 0.8125\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# train optimizer = AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "# define the loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for epoch in range(5):\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader):\n",
    "        out = model(input_ids=input_ids.to(device),\n",
    "                    attention_mask=attention_mask.to(device),\n",
    "                    token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "        loss = criterion(out, labels.to(device))\n",
    "        # backforward\n",
    "        loss.backward()\n",
    "        # use the optimizer\n",
    "        optimizer.step()\n",
    "        # reset the optimizer step to zero, can be put in front of backward\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if i % 50 == 0:\n",
    "            # softmax => get the max value index\n",
    "            out = out.argmax(dim=1)\n",
    "            accuracy = (out == labels.to(device)).sum().item() / len(labels.to(device))\n",
    "\n",
    "            print(epoch, i, loss.item(), accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "275dd1b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lansinuote--ChnSentiCorp-4d058ef86e3db8d5\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-4d058ef86e3db8d5/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd1803acb2c40229c7eea1605f1263e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[CLS] 床 发 出 吱 嘎 吱 嘎 的 声 音 ， 房 间 隔 [MASK] 太 差 ， 赠 送 的 早 餐 非 常 好 吃 。 [SEP]\n",
      "音 音\n",
      "1\n",
      "[CLS] 非 常 不 好 ， 我 们 渡 过 了 一 个 让 人 [MASK] 以 忍 受 的 纪 念 日. com / thread - 136 [SEP]\n",
      "难 难\n",
      "2\n",
      "[CLS] 定 的 商 务 大 床 房 ， 房 间 偏 小 了 ， [MASK] 过 经 济 性 酒 店 也 就 这 样 ； 环 境 [SEP]\n",
      "不 不\n",
      "3\n",
      "[CLS] 确 实 是 山 上 最 好 的 酒 店 ， 环 境 和 [MASK] 施 都 很 不 错 。 我 们 这 次 住 的 是 [SEP]\n",
      "设 设\n",
      "4\n",
      "[CLS] 这 本 书 紧 接 《 春 秋 大 义 》 ， 作 者 [MASK] 以 贯 之 地 以 浅 显 的 语 言 ， 告 诉 [SEP]\n",
      "一 一\n",
      "5\n",
      "[CLS] 非 常 不 满 这 酒 店 ， 配 不 上 5 星 。 [MASK] 一, 客 房 服 务 员 没 有 水 平, 房 [SEP]\n",
      "第 第\n",
      "6\n",
      "[CLS] 合 庆 的 商 务 单 间 可 以 堪 称 豪 华, [MASK] 施 特 别 先 进 ， 特 别 是 少 有 的 先 [SEP]\n",
      "设 设\n",
      "7\n",
      "[CLS] 这 是 我 住 过 的 最 差 的 酒 店 ， 房 间 [MASK] 味 难 闻 ， 刚 打 了 灭 蚊 药 水 ， 换 [SEP]\n",
      "气 气\n",
      "8\n",
      "[CLS] 总 体 很 满 意 ， 但 有 一 点 需 改 进 ， [MASK] 在 9 楼 入 住 ， 走 时 到 1 楼 前 台 [SEP]\n",
      "我 我\n",
      "9\n",
      "[CLS] 这 本 书 有 别 于 以 往 看 过 的 早 教 书 [MASK] ， 结 合 了 说 明 文 的 写 实 ， 散 文 [SEP]\n",
      "籍 籍\n",
      "10\n",
      "[CLS] [UNK] 用 起 来 不 习 惯 ， 速 度 慢 ， 分 区 [MASK] 烦 ， 带 了 很 多 垃 圾 软 件 ， 卸 载 [SEP]\n",
      "麻 麻\n",
      "11\n",
      "[CLS] 这 一 套 书 我 基 本 买 齐 了 ， 也 看 了 [MASK] 多 本 了 。 是 利 用 闲 暇 时 间 巩 固 [SEP]\n",
      "好 好\n",
      "12\n",
      "[CLS] 渡 假 村 周 围 景 色 不 错, 但 较 落 乡 [MASK] 硬 件 太 差, 服 务 水 准 有 待 提 高 [SEP]\n",
      ". .\n",
      "13\n",
      "[CLS] 相 对 来 说 ， 比 起 东 莞 ， 深 圳 的 酒 [MASK] ， 该 酒 店 服 务 质 量 还 是 略 有 小 [SEP]\n",
      "店 店\n",
      "14\n",
      "[CLS] 目 前 是 新 昌 最 好 的 酒 店 ， 环 境 和 [MASK] 务 质 量 都 非 常 好 ， 国 内 长 度 免 [SEP]\n",
      "服 服\n",
      "0.6875\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "        if i == 15:\n",
    "            break\n",
    "\n",
    "        print(i)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        out = out.argmax(dim=1)\n",
    "        correct += (out == labels).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "        print(token.decode(input_ids[0]))\n",
    "        print(token.decode(labels[0]), token.decode(labels[0]))\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
