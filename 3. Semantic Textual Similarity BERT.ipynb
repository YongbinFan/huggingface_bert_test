{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3362a434",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/35391/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-4d058ef86e3db8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.017606258392333984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Filter",
       "rate": null,
       "total": 9600,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/9600 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8001, '选择珠江花园的原因就是方便，有电动扶梯直', '接到达海边，周围餐馆、食廊、商场、超市、', 0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "\n",
    "#Get the data from hugginface\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, split):\n",
    "        dataset = load_dataset(path='lansinuote/ChnSentiCorp', split=split)\n",
    "\n",
    "        def f(data):\n",
    "            # only keep the senten length > 40\n",
    "            return len(data['text']) > 40\n",
    "\n",
    "        self.dataset = dataset.filter(f)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        text = self.dataset[i]['text']\n",
    "\n",
    "        # set the 1st sentence 0:20, 2nd sentence 20:40\n",
    "        sentence1 = text[:20]\n",
    "        sentence2 = text[20:40]\n",
    "        label = 0\n",
    "\n",
    "        # 50% chance get the no relationship data\n",
    "        if random.randint(0, 1) == 0:\n",
    "            # j => random index of the sentence\n",
    "            j = random.randint(0, len(self.dataset) - 1)\n",
    "            # get a pair of no relationship sentence, set label to 1\n",
    "            sentence2 = self.dataset[j]['text'][20:40]\n",
    "            label = 1\n",
    "\n",
    "        return sentence1, sentence2, label\n",
    "\n",
    "\n",
    "dataset = Dataset('train')\n",
    "\n",
    "sentence1, sentence2, label = dataset[0]\n",
    "\n",
    "len(dataset), sentence1, sentence2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9120b06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8001, '机器背面似乎被撕了张什么标签，残胶还在。', '一读难忘，欲罢不能。现代的都市生活，每天', 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence1, sentence2, label = dataset[4]\n",
    "\n",
    "len(dataset), sentence1, sentence2, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa19b7a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# use to(device) set the data and model to use cuda\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e70a58c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-chinese', vocab_size=21128, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# get the chinese version tokenizer\n",
    "token = BertTokenizer.from_pretrained('bert-base-chinese')\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e59695a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[CLS] 订 房 时 号 称 是 海 景 房, 结 果 入 住 是 3 楼, 别 说 [SEP] 海 景, 就 是 旁 边 的 绿 博 院 也 看 不 到. 好 在 白 天 [SEP] [PAD] [PAD]\n",
      "[CLS] 同 样 价 格 的 行 政 楼 层 房 间 有 大 有 小 ， 需 要 注 意 [SEP] 。 洗 手 间 装 修 布 置 的 很 舒 适 漂 亮 ， 有 淋 浴 和 浴 [SEP] [PAD] [PAD]\n",
      "[CLS] 生 下 孩 子 后 就 买 了 这 本 书 ， 觉 得 实 在 是 相 见 恨 [SEP] 晚 啊! 我 的 身 体 在 怀 孕 前 就 比 较 差 ， 也 是 辗 转 [SEP] [PAD] [PAD]\n",
      "[CLS] 这 是 我 第 一 次 通 过 当 当 网 买 书 ， 朋 友 说 这 里 买 [SEP] 我 非 常 不 能 理 解 的 是 ， 300 多 一 晚 的 酒 店 居 [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "[CLS] 我 也 是 看 了 评 论 才 买 的 ， 一 买 回 来 ， 儿 子 就 拿 [SEP] 起 荒 岛 历 险 来 看 ， 非 常 感 兴 趣 ， 问 他 为 什 么 好 [SEP] [PAD] [PAD]\n",
      "[CLS] 也 许 期 望 过 高 ， 拿 到 此 说 后 ， 读 了 一 部 分 觉 得 [SEP] ！ 做 工 十 分 精 致 ， 很 小 很 轻 便 ， 携 带 方 便 ！ 风 [SEP] [PAD] [PAD]\n",
      "[CLS] 在 同 档 次 的 笔 记 本 电 脑 中 性 价 比 高 ， 配 置 也 够 [SEP] 用 ， 品 牌 效 应 好 ， 外 观 不 错 ， 已 在 使 用 中 待 发 [SEP] [PAD] [PAD]\n",
      "[CLS] 开 机 20 分 钟 触 摸 板 能 烫 伤 手 指 ， 仔 细 检 查 风 [SEP] 看 完 。 当 时 我 的 奶 不 是 很 多 ， 宝 宝 总 要 吃 ， 家 [SEP] [PAD] [PAD] [PAD]\n",
      "torch.Size([8, 45]) torch.Size([8, 45]) torch.Size([8, 45]) tensor([0, 0, 0, 1, 0, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(data):\n",
    "    # sents = [(sent1,sent2).....]\n",
    "    sents = [i[:2] for i in data]\n",
    "    labels = [i[2] for i in data]\n",
    "\n",
    "    # encode the data\n",
    "    data = token.batch_encode_plus(batch_text_or_text_pairs=sents,\n",
    "                                   truncation=True,\n",
    "                                   padding='max_length',\n",
    "                                   max_length=45,\n",
    "                                   return_tensors='pt',\n",
    "                                   return_length=True,\n",
    "                                   add_special_tokens=True)\n",
    "\n",
    "    #input_ids:map the id to different words\n",
    "    #attention_mask: words == 1, padding  == 0 [1,1,1,1,1,0,0]\n",
    "    #token_type_ids: use for 1st sentence and 2nd sentence [0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "    input_ids = data['input_ids']\n",
    "    attention_mask = data['attention_mask']\n",
    "    token_type_ids = data['token_type_ids']\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    #print(data['length'], data['length'].max())\n",
    "\n",
    "    return input_ids, attention_mask, token_type_ids, labels\n",
    "\n",
    "\n",
    "# put the data into torch dataloader\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=8,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    print(len(loader))\n",
    "    for idx in range(len(input_ids)):\n",
    "        print(token.decode(input_ids[idx]))\n",
    "    print(input_ids.shape, attention_mask.shape, token_type_ids.shape, labels)\n",
    "    brea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f620d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 45, 768])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# get the pretrained model of bert, need the same version of the tokenizer\n",
    "pretrained = BertModel.from_pretrained('bert-base-chinese').to(device)\n",
    "\n",
    "# don't need train the model and update the params\n",
    "for param in pretrained.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# try the first batch, last_hidden_state will give the model's output\n",
    "out = pretrained(input_ids=input_ids.to(device),\n",
    "           attention_mask=attention_mask.to(device),\n",
    "           token_type_ids=token_type_ids.to(device))\n",
    "\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9151282d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define forward model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # set the Fully Connected Layer(fc layer)\n",
    "        # output willbe the softmax(token.vocab_size) which size should be 2(0/1)\n",
    "        self.fc = torch.nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        with torch.no_grad():\n",
    "            # do not need update the weight\n",
    "            out = pretrained(input_ids=input_ids.to(device),\n",
    "                             attention_mask=attention_mask.to(device),\n",
    "                             token_type_ids=token_type_ids.to(device))\n",
    "        # get the softmax value of the output\n",
    "        out = self.fc(out.last_hidden_state[:, 0])\n",
    "        out = out.softmax(dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "model(input_ids=input_ids,\n",
    "      attention_mask=attention_mask,\n",
    "      token_type_ids=token_type_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1bd44a7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5043620467185974 0.875\n",
      "100 0.62580806016922 0.625\n",
      "200 0.5098405480384827 0.75\n",
      "300 0.36935633420944214 1.0\n",
      "400 0.33695387840270996 1.0\n",
      "500 0.4939813017845154 0.875\n",
      "600 0.5929908752441406 0.75\n",
      "700 0.4678238332271576 0.875\n",
      "800 0.4044565260410309 1.0\n",
      "900 0.3614732623100281 1.0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# train optimizer = AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=5e-4)\n",
    "# define the loss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "model.train()\n",
    "for i, (input_ids, attention_mask, token_type_ids,\n",
    "        labels) in enumerate(loader):\n",
    "    out = model(input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "    loss = criterion(out, labels.to(device))\n",
    "    # backforward\n",
    "    loss.backward()\n",
    "    # use the optimizer\n",
    "    optimizer.step()\n",
    "    # reset the optimizer step to zero, can be put in front of backward\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        out = out.argmax(dim=1)\n",
    "        accuracy = (out == labels.to(device)).sum().item() / len(labels.to(device))\n",
    "        print(i, loss.item(), accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "275dd1b6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/35391/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--ChnSentiCorp-4d058ef86e3db8d5/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
      "Loading cached processed dataset at C:\\Users\\35391\\.cache\\huggingface\\datasets\\lansinuote___parquet\\lansinuote--ChnSentiCorp-4d058ef86e3db8d5\\0.0.0\\14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7\\cache-680cb52f3e4569f7.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0], device='cuda:0')\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
      "        1, 1, 0, 0, 1, 0, 1, 0])\n",
      "tensor([ True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False, False,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "1\n",
      "tensor([0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
      "        1, 0, 1, 1, 1, 0, 0, 0])\n",
      "tensor([ True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True,  True, False,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "2\n",
      "tensor([0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1], device='cuda:0')\n",
      "tensor([0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        1, 0, 0, 1, 0, 0, 1, 1])\n",
      "tensor([ True,  True,  True, False, False,  True,  True,  True,  True,  True,\n",
      "         True,  True,  True,  True,  True, False,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "3\n",
      "tensor([1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1], device='cuda:0')\n",
      "tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 1, 0, 1])\n",
      "tensor([ True,  True,  True, False, False,  True, False,  True,  True,  True,\n",
      "        False,  True,  True,  True,  True,  True,  True, False,  True,  True,\n",
      "         True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True], device='cuda:0')\n",
      "4\n",
      "tensor([0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 0], device='cuda:0')\n",
      "tensor([0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
      "        1, 1, 0, 1, 0, 1, 1, 1])\n",
      "tensor([ True, False, False,  True, False,  True,  True,  True,  True, False,\n",
      "         True, False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True,  True, False,  True,  True,  True,  True,  True,  True,  True,\n",
      "         True, False], device='cuda:0')\n",
      "10\n",
      "20\n",
      "0.88125\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "def test():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    loader_test = torch.utils.data.DataLoader(dataset=Dataset('test'),\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn,\n",
    "                                              shuffle=True,\n",
    "                                              drop_last=True)\n",
    "\n",
    "    for i, (input_ids, attention_mask, token_type_ids,\n",
    "            labels) in enumerate(loader_test):\n",
    "\n",
    "#         if i == 5:\n",
    "#             break\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = model(input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids)\n",
    "\n",
    "        pred = out.argmax(dim=1)\n",
    "        \n",
    "        if i<5:\n",
    "            print(i)\n",
    "            print(pred)\n",
    "            print(labels)\n",
    "            print(pred == labels.to(device))\n",
    "        else:\n",
    "            if i % 10 == 0:\n",
    "                print(i)\n",
    "\n",
    "        correct += (pred == labels.to(device)).sum().item()\n",
    "        total += len(labels)\n",
    "\n",
    "    print(correct / total)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b181e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
